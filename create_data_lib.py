# -*- coding: utf-8 -*-
DEBUG = True
"""
faiss_topk_search.py â€” ã‚³ãƒ¼ãƒ‘ã‚¹ã®å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’ FAISS ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã—ã€
é•·æ–‡ã‚¯ã‚¨ãƒªï¼ˆãƒãƒ£ãƒ³ã‚¯åŒ–ï¼‰ã‹ã‚‰é¡ä¼¼ Top-K æ–‡æ›¸ã® (doc_id, score) ã‚’è¿”ã™ã€‚
æ—¢å­˜ calc_faiss.py ã®å®Ÿè£…æ–¹é‡ã‚’æœ€å¤§é™è¸è¥²ã€‚

ä½¿ã„æ–¹ï¼ˆå˜ä¸€ã‚¯ã‚¨ãƒªï¼‰
python faiss_topk_search.py \
  --corpus_dir /path/to/corpus_texts \
  --query /path/to/query.txt \
  --topk 10 \
  --max_tokens 2048 --stride_tokens 1536 \
  --per_chunk_topk 8 \
  --reduce_mode topk-mean --reduce_topk 4 \
  [--model_name sentence-transformers/all-MiniLM-L6-v2] \
  [--out_json results.json]

è¤‡æ•°ã‚¯ã‚¨ãƒª
python faiss_topk_search.py \
  --corpus_dir /path/to/corpus_texts \
  --query_dir /path/to/queries \
  --topk 10 --out_csv results.csv
"""
import os
import re
import json
import csv
import argparse
from collections import defaultdict
from typing import Callable, Generator, Iterable, List, Optional, Tuple
from pathlib import Path
import shutil
from dotenv import load_dotenv
import pandas as pd
from tqdm import tqdm

try:
    import faiss  # type: ignore
except Exception as e:
    raise RuntimeError("faiss ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`pip install faiss-gpu` ãªã©ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚") from e


# ===================== 1) å‰å‡¦ç†ãƒ»I/O =====================

def _prep_texts(texts: List[str]) -> List[str]:
    """è»½é‡ãªå‰å‡¦ç†ï¼ˆç©ºç™½æ­£è¦åŒ–ãƒ»ãƒˆãƒªãƒ ï¼‰ã€‚"""
    out = []
    for t in texts:
        t = "" if t is None else str(t)
        t = re.sub(r"\s+", " ", t).strip()
        out.append(t)
    return out

def list_txt_files(dir_path: str) -> List[str]:
    fs = [f for f in os.listdir(dir_path) if f.lower().endswith(".txt")]
    fs.sort(key=lambda x: (len(os.path.splitext(x)[0]), os.path.splitext(x)[0]))
    return fs

def read_text(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()

def basename_wo_ext(p: str) -> str:
    return os.path.splitext(os.path.basename(p))[0]


# ===================== 2) ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨ãƒãƒ£ãƒ³ã‚¯åŒ– =====================

def default_char_tokenize(s: str) -> List[str]:
    """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼š1æ–‡å­—=1ãƒˆãƒ¼ã‚¯ãƒ³ã€‚å¿…è¦ã«å¿œã˜ã¦å·®ã—æ›¿ãˆå¯ã€‚"""
    return list(s or "")

def chunk_by_tokens(
    text: str,
    tokenize_fn: Callable[[str], List[str]],
    max_tokens: int,
    stride_tokens: Optional[int] = None,
) -> List[str]:
    """ãƒˆãƒ¼ã‚¯ãƒ³é•·ãƒ™ãƒ¼ã‚¹ã®ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ãƒãƒ£ãƒ³ã‚¯åŒ–ã€‚"""
    if stride_tokens is None:
        stride_tokens = int(max_tokens * 0.75)

    toks = tokenize_fn(text)
    n = len(toks)
    if n == 0:
        return [""]

    chunks: List[str] = []
    start = 0
    while start < n:
        end = min(start + max_tokens, n)
        chunk = "".join(toks[start:end])
        chunks.append(chunk)
        if end == n:
            break
        start += stride_tokens
    return chunks


# ===================== 3) åŸ‹ã‚è¾¼ã¿ï¼ˆTorchï¼‰ =====================
from typing import List, Optional, Callable, Any
import numpy as np
import torch
import torch.nn as nn

def _default_char_tokenize(text: str) -> List[str]:
    return list(text or "")

class TorchTextEmbedder:
    """
    å„ªå…ˆé †:
      1) Sentence-Transformersï¼ˆ.encodeï¼‰
      2) GoogleGenerativeAIEmbeddingsï¼ˆ.embed_documentsï¼‰
      3) ãƒãƒƒã‚·ãƒ¥å¹³å‡ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    å…±é€šI/F:
      encode(texts: List[str], batch_size: int) -> np.ndarray[float32]
    """
    def __init__(
        self,
        model_name: Optional[str] = None,
        device: Optional[str] = None,
        dim: int = 768,
        vocab_size: int = 200_003,
        tokenize_fn: Callable[[str], List[str]] = _default_char_tokenize,
    ):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.dim = dim
        self.vocab_size = vocab_size
        self.tokenize_fn = tokenize_fn

        self._backend: Any = None
        self._backend_kind: str = "hash"  # "st" | "google" | "hash"

        # ã¾ãšæ˜ç¤ºæŒ‡å®šãŒGoogleç³»ã ã£ãŸã‚‰Googleã‚’è©¦ã™
        if self._try_setup_google(model_name):
            self._backend_kind = "google"
        elif self._try_setup_st(model_name):
            self._backend_kind = "sentence_transformer"
        else:
            self._setup_hash_backend()

    # ---- Backends setup ----
    def _try_setup_st(self, model_name: Optional[str]) -> bool:
        try:
            from sentence_transformers import SentenceTransformer  # type: ignore
        except Exception:
            return False
        try:
            name = model_name or "sentence-transformers/all-MiniLM-L6-v2"
            self._backend = SentenceTransformer(name, device=self.device)
            # max_seq_length ã¯ãƒ¢ãƒ‡ãƒ«ä¾å­˜ãªã®ã§ã‚€ã‚„ã¿ã«æ‹¡å¼µã—ãªã„
            return True
        except Exception as e:
            print(f"âš ï¸ Sentence-Transformers åˆæœŸåŒ–å¤±æ•—: {e}")
            self._backend = None
            return False

    def _try_setup_google(self, model_name: str) -> bool:
        try:
            # from langchain_google_genai import GoogleGenerativeAIEmbeddings  # type: ignore
            print("comment out : from langchain_google_genai import GoogleGenerativeAIEmbeddings")
        except Exception as e:
            print(f"âš ï¸ langchain_google_genai import å¤±æ•—: {e}")
            return False
        try:
            # GOOGLE_API_KEY ã¯ç’°å¢ƒå¤‰æ•°ã§è¨­å®šã—ã¦ãŠã
            self._backend = GoogleGenerativeAIEmbeddings(model=model_name)
            # æ³¨æ„: max_seq_length ã¯å¤–éƒ¨åˆ¶å¾¡ã§ããªã„ â†’ å‘¼ã³å‡ºã—å´ã§ãƒãƒ£ãƒ³ã‚¯åŒ–ã—ã¦æ¸¡ã™
            return True
        except Exception as e:
            print(f"âš ï¸ GoogleGenerativeAIEmbeddings åˆæœŸåŒ–å¤±æ•—: {e}")
            self._backend = None
            return False

    def _setup_hash_backend(self) -> None:
        g = torch.Generator().manual_seed(42)
        self._emb_vocab = int(self.vocab_size)  # â˜… å‰å›ã® AttributeError ã®åŸå› ã«å¯¾å‡¦
        self._emb = nn.Embedding(self._emb_vocab, self.dim)
        torch.nn.init.normal_(self._emb.weight, mean=0.0, std=0.02, generator=g)
        self._emb.to(self.device).eval()

    # ---- Public API ----
    @torch.inference_mode()
    def encode(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        texts = _prep_texts(texts)
        if not texts:
            return np.zeros((0, self._infer_dim()), dtype=np.float32)

        if self._backend is not None:
            if self._backend_kind == "sentence_transformer":
                return self._encode_st(texts, batch_size)
            elif self._backend_kind == "google":
                return self._encode_google(texts, batch_size)

        return self._encode_hash(texts, batch_size)

    # ---- encode implementations ----
    def _encode_st(self, texts: List[str], batch_size: int) -> np.ndarray:
        X = self._backend.encode(
            texts,
            batch_size=batch_size,
            convert_to_numpy=True,
            normalize_embeddings=False,
            show_progress_bar=False,
        )
        return np.asarray(X, dtype=np.float32)

    def _encode_google(self, texts: List[str], batch_size: int) -> np.ndarray:
        outs = []
        n = len(texts)
        for i in range(0, n, max(1, batch_size)):
            batch = texts[i:i+batch_size]
            X = self._backend.embed_documents(batch)  # -> List[List[float]]
            outs.append(np.asarray(X, dtype=np.float32))
        return np.vstack(outs)

    def _encode_hash(self, texts: List[str], batch_size: int) -> np.ndarray:
        outs = []
        for i in range(0, len(texts), max(1, batch_size)):
            sub = texts[i: i + batch_size]
            vecs = []
            for s in sub:
                toks = self.tokenize_fn(s) or [""]
                ids = [(hash(tok) % self._emb_vocab) for tok in toks]
                tt = torch.tensor(ids, dtype=torch.long, device=self.device)
                e = self._emb(tt)                  # (L, D)
                v = e.mean(dim=0, keepdim=True)    # (1, D)
                vecs.append(v)
            V = torch.cat(vecs, dim=0).detach().cpu().numpy().astype(np.float32)
            outs.append(V)
        return np.vstack(outs)

    def _infer_dim(self) -> int:
        if self._backend_kind == "st":
            try:
                return int(self._backend.get_sentence_embedding_dimension())
            except Exception:
                return int(self.dim)
        elif self._backend_kind == "google":
            try:
                X = self._backend.embed_documents(["test"])
                return int(len(X[0]))
            except Exception:
                return int(self.dim)
        else:
            return int(self.dim)


# ===================== 4) é¡ä¼¼åº¦è£œåŠ© =====================

def l2_normalize_inplace(x: np.ndarray):
    if x.size == 0:
        return
    faiss.normalize_L2(x)

def build_ip_index(d: int, use_gpu: bool = True):
    cpu_index = faiss.IndexFlatIP(d)

    # depending in torch.cuda
    use_gpu = True
    if use_gpu and torch.cuda.is_available():
        res = faiss.StandardGpuResources()
        return faiss.index_cpu_to_gpu(res, 0, cpu_index)
    return cpu_index


# ===================== 5) ã‚³ãƒ¼ãƒ‘ã‚¹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ– =====================

def build_corpus_index(
    corpus_dir: str,
    embedder: TorchTextEmbedder,
    tokenize_fn: Callable[[str], List[str]] = default_char_tokenize,
    max_tokens: int = 2048,
    stride_tokens: Optional[int] = None,
    emb_batch_size: int = 32,
    use_gpu_index: bool = True,
):
    """
    è¿”ã‚Šå€¤:
      index: FAISS IndexFlatIP (GPUåŒ–ã•ã‚Œã¦ã„ã‚‹å ´åˆã‚ã‚Š)
      chunk_docids: np.ndarray[int32]  # ãƒ™ã‚¯ãƒˆãƒ«ã”ã¨ã® doc è¡Œç•ªå·ï¼ˆ0..M-1ï¼‰
      doc_bases: List[str]             # è¡Œç•ªå·â†’ãƒ™ãƒ¼ã‚¹åï¼ˆ"1","2",...ï¼‰
    """
    files = list_txt_files(corpus_dir)
    if not files:
        raise ValueError(f"ã‚³ãƒ¼ãƒ‘ã‚¹ãŒç©ºã§ã™: {corpus_dir}")

    doc_bases: List[str] = [basename_wo_ext(f) for f in files]
    all_vecs: List[np.ndarray] = []
    chunk_docids: List[int] = []

    pbar = tqdm(total=len(files), dynamic_ncols=True, desc="Processing", unit="file", smoothing=0)

    # 1ãƒ‘ã‚¹ã§é †æ¬¡ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆå·¨å¤§ã‚³ãƒ¼ãƒ‘ã‚¹ã§ã¯é©å®œåˆ†å‰²è¿½åŠ ã«å¤‰æ›´å¯ï¼‰
    for di, fname in enumerate(files):
        pbar.update(1)  # ã‚¹ã‚­ãƒƒãƒ—ã§ã‚‚1ä»¶å‰é€²
        path = os.path.join(corpus_dir, fname)
        text = read_text(path)
        chunks = chunk_by_tokens(text, tokenize_fn, max_tokens, stride_tokens)
        V = embedder.encode(chunks, batch_size=emb_batch_size)  # (mi, d)
        if V.size == 0:
            continue
        l2_normalize_inplace(V)
        all_vecs.append(V)
        chunk_docids.extend([di] * V.shape[0])

    if not all_vecs:
        raise ValueError("å…¨ã‚³ãƒ¼ãƒ‘ã‚¹ã§åŸ‹ã‚è¾¼ã¿ãŒç©ºã§ã—ãŸã€‚")

    X = np.vstack(all_vecs).astype(np.float32)     # (N, d)
    chunk_docids = np.asarray(chunk_docids, dtype=np.int32)
    index = build_ip_index(X.shape[1], use_gpu=use_gpu_index)
    index.add(X)
    return index, chunk_docids, doc_bases


# ===================== 6) ã‚¯ã‚¨ãƒªæ¤œç´¢ï¼ˆdocé›†ç´„ topkï¼‰ =====================

def aggregate_scores_per_doc(
    indices: np.ndarray,      # (Q, kq)
    dists: np.ndarray,        # (Q, kq)  å†…ç©=ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼ˆL2æ­£è¦åŒ–æ¸ˆï¼‰
    chunk_docids: np.ndarray, # (N,)
    reduce_mode: str = "topk-mean",
    reduce_topk: int = 4,
) -> List[Tuple[int, float]]:
    """
    FAISSæ¤œç´¢ã§å¾—ãŸ (indices, dists) ã‚’ doc å˜ä½ã«é›†ç´„ã—ã€(doc_id, score) ã‚’è¿”ã™ã€‚
    """
    # doc -> é¡ä¼¼åº¦ã®ãƒªã‚¹ãƒˆ
    scores_by_doc: defaultdict[int, List[float]] = defaultdict(list)
    Q, kq = dists.shape
    for qi in range(Q):
        for kj in range(kq):
            vec_idx = int(indices[qi, kj])
            if vec_idx < 0:
                continue
            doc_id = int(chunk_docids[vec_idx])
            scores_by_doc[doc_id].append(float(dists[qi, kj]))

    # é›†ç´„
    results: List[Tuple[int, float]] = []
    for doc_id, vals in scores_by_doc.items():
        if not vals:
            continue
        arr = np.asarray(vals, dtype=np.float32)
        if reduce_mode == "max":
            score = float(arr.max())
        elif reduce_mode == "mean":
            score = float(arr.mean())
        elif reduce_mode == "topk-mean":
            k = min(reduce_topk, arr.size)
            score = float(np.partition(arr, -k)[-k:].mean())
        else:
            raise ValueError(f"unknown reduce_mode: {reduce_mode}")
        results.append((doc_id, score))

    # ã‚¹ã‚³ã‚¢é™é †
    results.sort(key=lambda x: x[1], reverse=True)
    return results

def search_topk_for_query(
    index,
    chunk_docids: np.ndarray,
    doc_bases: List[str],
    query_text: str,
    embedder: TorchTextEmbedder,
    tokenize_fn: Callable[[str], List[str]] = default_char_tokenize,
    max_tokens: int = 2048,
    stride_tokens: Optional[int] = None,
    emb_batch_size: int = 32,
    per_chunk_topk: int = 8,       # å„ã‚¯ã‚¨ãƒªãƒãƒ£ãƒ³ã‚¯ã§å–å¾—ã™ã‚‹è¿‘å‚æ•°
    reduce_mode: str = "topk-mean",
    reduce_topk: int = 4,
    topk: int = 10,                # doc å˜ä½ã®æœ€çµ‚ Top-K
) -> List[Tuple[str, float]]:
    # ã‚¯ã‚¨ãƒªã‚’ãƒãƒ£ãƒ³ã‚¯åŒ–â†’åŸ‹ã‚è¾¼ã¿â†’æ­£è¦åŒ–
    q_chunks = chunk_by_tokens(query_text, tokenize_fn, max_tokens, stride_tokens)
    Q = embedder.encode(q_chunks, batch_size=emb_batch_size)  # (q, d)
    if Q.size == 0:
        return []
    l2_normalize_inplace(Q)

    # è¿‘å‚æ¢ç´¢ï¼ˆIndexFlatIPï¼‰
    per_chunk_topk = max(1, per_chunk_topk)
    per_chunk_topk = min(per_chunk_topk, index.ntotal if hasattr(index, "ntotal") else per_chunk_topk)
    D, I = index.search(Q, per_chunk_topk)  # D: (q, kq)

    # doc å˜ä½ã«é›†ç´„
    doc_scores = aggregate_scores_per_doc(
        indices=I, dists=D,
        chunk_docids=chunk_docids,
        reduce_mode=reduce_mode,
        reduce_topk=reduce_topk,
    )
    # ä¸Šä½ topk ã‚’ doc_base åã¨ã‚¹ã‚³ã‚¢ã§è¿”ã™
    out: List[Tuple[str, float]] = []
    for doc_id, sc in doc_scores[:topk]:
        out.append((doc_bases[doc_id], float(sc)))
    return out


# ===================== 7) CLI =====================

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--corpus_dir", required=True, help="ã‚³ãƒ¼ãƒ‘ã‚¹ .txt ç¾¤ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆ1.txt, 2.txt, ...ï¼‰")
    gq = ap.add_mutually_exclusive_group(required=True)
    gq.add_argument("--query", help="å˜ä¸€ã‚¯ã‚¨ãƒª .txt")
    gq.add_argument("--query_dir", help="è¤‡æ•°ã‚¯ã‚¨ãƒª .txt ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    ap.add_argument("--model_name", default=None, help="sentence-transformers ã®ãƒ¢ãƒ‡ãƒ«åï¼ˆçœç•¥å¯ï¼‰")
    ap.add_argument("--max_tokens", type=int, default=2048)
    ap.add_argument("--stride_tokens", type=int, default=None)
    ap.add_argument("--emb_batch_size", type=int, default=32)
    ap.add_argument("--use_gpu_index", action="store_true", help="FAISSã‚’GPUåŒ–ï¼ˆå¯èƒ½ãªã‚‰ï¼‰")
    ap.add_argument("--per_chunk_topk", type=int, default=2048, help="ã‚¯ã‚¨ãƒªå„ãƒãƒ£ãƒ³ã‚¯ã§å–å¾—ã™ã‚‹è¿‘å‚æ•°")
    ap.add_argument("--reduce_mode", choices=["max", "mean", "topk-mean"], default="topk-mean")
    ap.add_argument("--reduce_topk", type=int, default=4, help="reduce_mode=topk-mean ã® k")
    ap.add_argument("--topk", type=int, default=10, help="æœ€çµ‚å‡ºåŠ›ã® doc Top-K")
    ap.add_argument("--out_json", default=None, help="å˜ä¸€ã‚¯ã‚¨ãƒªã®çµæœã‚’ JSON ã§ä¿å­˜")
    ap.add_argument("--out_csv", default=None, help="è¤‡æ•°ã‚¯ã‚¨ãƒªã®çµæœã‚’ CSV ã§ä¿å­˜ï¼ˆquery_id,doc_id,score,rankï¼‰")
    args = ap.parse_args()

    # åŸ‹ã‚è¾¼ã¿å™¨
    embedder = TorchTextEmbedder(model_name=args.model_name)

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
    print("ğŸ”§ Building corpus index ...")
    index, chunk_docids, doc_bases = build_corpus_index(
        corpus_dir=args.corpus_dir,
        embedder=embedder,
        tokenize_fn=default_char_tokenize,
        max_tokens=args.max_tokens,
        stride_tokens=args.stride_tokens,
        emb_batch_size=args.emb_batch_size,
        use_gpu_index=args.use_gpu_index,
    )
    print(f"âœ… Index built: vectors={getattr(index, 'ntotal', 'N/A')}, docs={len(doc_bases)}")

    # å˜ä¸€ã‚¯ã‚¨ãƒª
    if args.query:
        qtxt = read_text(args.query)
        results = search_topk_for_query(
            index=index,
            chunk_docids=chunk_docids,
            doc_bases=doc_bases,
            query_text=qtxt,
            embedder=embedder,
            tokenize_fn=default_char_tokenize,
            max_tokens=args.max_tokens,
            stride_tokens=args.stride_tokens,
            emb_batch_size=args.emb_batch_size,
            per_chunk_topk=args.per_chunk_topk,
            reduce_mode=args.reduce_mode,
            reduce_topk=args.reduce_topk,
            topk=args.topk,
        )
        print("=== Top-K ===")
        for rnk, (doc_id, score) in enumerate(results, 1):
            print(f"{rnk:>2}: doc={doc_id}  score={score:.6f}")

        if args.out_json:
            os.makedirs(os.path.dirname(args.out_json), exist_ok=True)
            with open(args.out_json, "w", encoding="utf-8") as fw:
                json.dump(
                    [{"rank": i+1, "doc_id": d, "score": float(s)} for i, (d, s) in enumerate(results)],
                    fw, ensure_ascii=False, indent=2
                )
            print(f"ğŸ“ wrote: {args.out_json}")

    # è¤‡æ•°ã‚¯ã‚¨ãƒª
    else:
        qfiles = list_txt_files(args.query_dir)
        if not qfiles:
            raise ValueError(f"ã‚¯ã‚¨ãƒªãŒç©ºã§ã™: {args.query_dir}")

        rows: List[Tuple[str, str, float, int]] = []  # (query_id, doc_id, score, rank)

        pbar = tqdm(total=len(qfiles), dynamic_ncols=True, desc="query Processing", unit="file", smoothing=0)

        for i, qf in enumerate(qfiles):
            pbar.update(1)  # ã‚¹ã‚­ãƒƒãƒ—ã§ã‚‚1ä»¶å‰é€²
            if i < 3942:
                continue
            qid = basename_wo_ext(qf)
            qtxt = read_text(os.path.join(args.query_dir, qf))
            if i < 3942:
                if i >3940:
                    print(i, len(qtxt))
                continue
            if i < 8559:
                if i >8557:
                    print(i, len(qtxt))
                continue

            results = search_topk_for_query(
                index=index,
                chunk_docids=chunk_docids,
                doc_bases=doc_bases,
                query_text=qtxt,
                embedder=embedder,
                tokenize_fn=default_char_tokenize,
                max_tokens=args.max_tokens,
                stride_tokens=args.stride_tokens,
                emb_batch_size=args.emb_batch_size,
                per_chunk_topk=args.per_chunk_topk,
                reduce_mode=args.reduce_mode,
                reduce_topk=args.reduce_topk,
                topk=args.topk,
            )
            for rnk, (doc_id, score) in enumerate(results, 1):
                rows.append((qid, doc_id, float(score), rnk))

            df = pd.DataFrame(rows, columns=["q_id", "doc_id", "cos_sim", "rank"])
            filename = qid
            file_path = CFG.RESULT_RANK_ROOT / (filename + ".csv")
            df.to_csv(file_path,
                      index=False,  # DataFrameã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹(0, 1, 2...)ã‚’CSVã«å«ã‚ãªã„
                      encoding='utf-8-sig')

        # # å‡ºåŠ›
        # if args.out_csv:
        #     os.makedirs(os.path.dirname(args.out_csv), exist_ok=True)
        #     with open(args.out_csv, "w", encoding="utf-8", newline="") as fw:
        #         w = csv.writer(fw)
        #         w.writerow(["query_id", "doc_id", "score", "rank"])
        #         for qid, did, sc, rnk in rows:
        #             w.writerow([qid, did, f"{sc:.6f}", rnk])
        #     print(f"ğŸ“ wrote: {args.out_csv}")
        # else:
        #     # ç”»é¢è¡¨ç¤ºï¼ˆå„ã‚¯ã‚¨ãƒªã”ã¨ï¼‰
        #     cur_qid = None
        #     for qid, did, sc, rnk in rows:
        #         if qid != cur_qid:
        #             cur_qid = qid
        #             print(f"\n=== Query: {qid} ===")
        #         print(f"{rnk:>2}: doc={did}  score={sc:.6f}")



# config.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ«ãƒ¼ãƒˆã«ä½œã£ã¦ã€APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„
# LANGSMITH_API_KEY=api_key
# GOOGLE_API_KEY=api_key

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
load_dotenv(dotenv_path="config.env")
OUTPUT_ROOT_DIR = os.environ.get("OUTPUT_ROOT")

# import os
os.environ["LANGSMITH_TRACING"] = "true"

class Config():

    CSV = "1" # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®CSVã®ç•ªå·
    REF_CONT = "ref" # ç´ã¥ã

    input_folder_name = "gr"
    output_folder_name = "text"
    output_folder_a = f'{OUTPUT_ROOT_DIR}/graph/csv{CSV}/{REF_CONT}/{output_folder_name}{CSV}_a'  # å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹ã‚’æŒ‡å®š
    output_folder_b = f'{OUTPUT_ROOT_DIR}/graph/csv{CSV}/{REF_CONT}/{output_folder_name}{CSV}_b'  # å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹ã‚’æŒ‡å®š

    REF_CONT = "cont" # ç´ã¥ã‹ãªã„ã€åˆ†é¡ã‚³ãƒ¼ãƒ‰ï¼”æ¡ãŒå®Œå…¨ä¸€è‡´
    output_folder_cont = f'{OUTPUT_ROOT_DIR}/graph/csv{CSV}/{REF_CONT}/{output_folder_name}{CSV}_b'  # å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹ã‚’æŒ‡å®š


    OUTPUT_ROOT_A=Path(output_folder_a).resolve()
    OUTPUT_ROOT_B=Path(output_folder_b).resolve()
    OUTPUT_ROOT_CONT=Path(output_folder_cont).resolve()

    OUTPUT_ROOT_A.mkdir(parents=True, exist_ok=True)
    OUTPUT_ROOT_B.mkdir(parents=True, exist_ok=True)
    OUTPUT_ROOT_CONT.mkdir(parents=True, exist_ok=True)


    TEXT_A_DIR = f"{OUTPUT_ROOT_DIR}/graph/csv{CSV}/{REF_CONT}/{input_folder_name}{CSV}_a"
    TEXT_B_DIR = f"{OUTPUT_ROOT_DIR}/graph/csv{CSV}/{REF_CONT}/{input_folder_name}{CSV}_b"
    TEXT_A_ROOT = Path(TEXT_A_DIR).resolve()
    TEXT_B_ROOT = Path(TEXT_B_DIR).resolve()

    # ger all *.txt files in TEXT_A_DIR and TEXT_B_DIR and sort them
    TEXT_A_FILES = sorted(list(Path(TEXT_A_DIR).glob("*.pkl")))
    TEXT_B_FILES = sorted(list(Path(TEXT_B_DIR).glob("*.pkl")))

    # ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ç™»éŒ²ã™ã‚‹è«‹æ±‚é …ï¼‘ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç½®ãã€‚ãƒ•ã‚¡ã‚¤ãƒ«åã¯å…¬é–‹æ–‡æ›¸ID
    corpus_folder = f"{OUTPUT_ROOT_DIR}/graph/csv{CSV}/corpus"
    CORPUS_ROOT = Path(corpus_folder).resolve()
    CORPUS_ROOT.mkdir(parents=True, exist_ok=True)

    query_folder = f"{OUTPUT_ROOT_DIR}/graph/csv{CSV}/query"
    QUERY_ROOT = Path(query_folder).resolve()
    QUERY_ROOT.mkdir(parents=True, exist_ok=True)

    out_csv = f"{OUTPUT_ROOT_DIR}/graph/csv{CSV}/results/topk_results_{CSV}.csv"
    # divide files into 3 parts

    result_rank_folder = f"{OUTPUT_ROOT_DIR}/graph/csv{CSV}/result_rank"
    RESULT_RANK_ROOT = Path(result_rank_folder).resolve()
    RESULT_RANK_ROOT.mkdir(parents=True, exist_ok=True)

    total_files = 6000
    num_divide = 3
    files_per_part = total_files // num_divide

CFG = Config()

CREATE_DATA = False

def configure_faiss():
    global CFG
    # FAISSã®è¨­å®šï¼ˆå¿…è¦ã«å¿œã˜ã¦èª¿æ•´ï¼‰
    # faiss.omp_set_num_threads(4)
    # faiss.omp_set_num_threads(1)  # ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰ã«ã—ãŸã„å ´åˆ
    # faiss.downcast_Index(faiss.IndexFlatIP(128)).metric_type = faiss.METRIC_L2  # è·é›¢é–¢æ•°å¤‰æ›´ä¾‹

    df = pd.read_csv(f"{OUTPUT_ROOT_DIR}/graph/csv1/ref/df_concat1.csv")
    df_cont = pd.read_csv(f"{OUTPUT_ROOT_DIR}/graph/csv1/cont/df_concat1.csv")
    # dfã®syutuganåˆ—ã‚’å–å¾—
    claim_column = df["syutugan"].tolist()
    ref_column = df["himotuki"].tolist()
    ref_cont_column = df_cont["himotuki"].tolist()

    CFG.total_files = len(df)

    if CREATE_DATA:
    # ãƒ‡ãƒ¼ã‚¿ä½œæˆç”¨
        pbar = tqdm(total=CFG.total_files, dynamic_ncols=True, desc="Processing", unit="file", smoothing=0)

        for i in range(CFG.total_files):
            pbar.update(1)  # ã‚¹ã‚­ãƒƒãƒ—ã§ã‚‚1ä»¶å‰é€²

            p = CFG.OUTPUT_ROOT_B / f"{i}.txt"
            if not p.exists():
                continue
            target_file_name = ref_column[i]

            target = CFG.CORPUS_ROOT / f"{target_file_name}.txt"
            shutil.copy(p, target)

            p = CFG.OUTPUT_ROOT_A / f"{i}.txt"
            if not p.exists():
                continue
            target_file_name = claim_column[i]

            target = CFG.CORPUS_ROOT / f"{target_file_name}.txt"
            shutil.copy(p, target)

            target = CFG.QUERY_ROOT / f"{target_file_name}.txt"
            shutil.copy(p, target)

        #
        # for i in range(CFG.total_files):
        #     p = CFG.QUERY_ROOT / f"{i}.txt"
        #     if not p.exists():
        #         continue
        #     target_file_name = claim_column[i]
        #
        #     target = CFG.QUERY_ROOT / f"{target_file_name}.txt"
        #     shutil.copy(p, target)

if __name__ == "__main__":
    if CREATE_DATA:
        configure_faiss()
    import sys
    # å®Ÿè¡Œå¼•æ•°ã®è¨­å®š
    sys.argv = [
        "top_k.py",
        "--corpus_dir", CFG.corpus_folder,
        "--query_dir", CFG.query_folder,
        "--topk", "100",
        "--max_tokens", "4096",
        "--stride_tokens", "512",
        "--out_csv", CFG.out_csv,
        # "--model_name", "models/text-embedding-004",
        # "--model_name", "models/gemini-embedding-001"
        "--model_name", "sentence-transformers/all-MiniLM-L6-v2"
    ]
    main()
